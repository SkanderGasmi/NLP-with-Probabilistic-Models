{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings \n",
    "\n",
    "In this notebook, we will practice how to compute word embeddings and use them for sentiment analysis.\n",
    "- To implement sentiment analysis, we can go beyond counting the number of positive words and negative words. \n",
    "- we can find a way to represent each word numerically, by a vector. \n",
    "- The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures. \n",
    "\n",
    "In this notebook, we will explore a classic way of generating word embeddings or representations.\n",
    "- we will implement the famous model called the continuous bag of words (CBOW) model introduced in this paper (`Efficient Estimation of Word Representations in Vector Space`)[https://arxiv.org/abs/1301.3781]. \n",
    "\n",
    "We will:\n",
    "\n",
    "- Train word vectors from scratch.\n",
    "- Learn how to create batches of data.\n",
    "- Understand how backpropagation works.\n",
    "- Plot and visualize the learned word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - The Continuous Bag of Words Model](#1)\n",
    "- [2 - Training the Model](#2)\n",
    "    - [2.1 - Initializing the Model](#2.1)\n",
    "        - [initialize_model ](#ex-1)\n",
    "    - [2.2 - Softmax](#2.2)\n",
    "        - [softmax](#ex-2)\n",
    "    - [2.3 - Forward Propagation](#2.3)\n",
    "        - [forward_prop](#ex-3)\n",
    "    - [2.4 - Cost Function](#2.4)\n",
    "    - [2.5 - Training the Model - Backpropagation](#2.5)\n",
    "        - [back_prop ](#ex-4)\n",
    "    - [2.6 - Gradient Descent](#2.6)\n",
    "        - [gradient_descent](#ex-5)\n",
    "- [3 - Visualizing the Word Vectors](#3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - The Continuous Bag of Words Model\n",
    "\n",
    "Let's take a look at the following sentence: \n",
    ">**'I am happy because I am learning'**. \n",
    "\n",
    "- In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\n",
    "- For example, if you were to choose a context half-size of say $C = 2$, then we would try to predict the word **happy** given the context that includes 2 words before and 2 words after the center word:\n",
    "\n",
    "> $C$ words before: [I, am] \n",
    "\n",
    "> $C$ words after: [because, I] \n",
    "\n",
    "- In other words:\n",
    "\n",
    "$$context = [I,am, because, I]$$\n",
    "$$target = happy$$\n",
    "\n",
    "The structure of our model will look like this:\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/word2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 1 </div>\n",
    "\n",
    "Where $\\bar x$ is the average of all the one hot vectors of the context words. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/mean_vec2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 2 </div>\n",
    "\n",
    "Once we have encoded all the context words, we can use $\\bar x$ as the input to our model. \n",
    "\n",
    "The architecture of the neural network we will be implementing is as follows:\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " a &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
    " \\hat y &= softmax(z)   \\tag{4} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Python libraries and helper functions (in utils2) \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
    "import w4_unittest\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 60976 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
     ]
    }
   ],
   "source": [
    "# Load, tokenize and process the data\n",
    "import re                                                           #  Load the Regex-modul\n",
    "with open('./data/shakespeare.txt') as f:\n",
    "    data = f.read()                                                 #  Read in the data\n",
    "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
    "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping words to indices and indices to words\n",
    "`get_dict` is implemented in `utils.py` and it creates a dictionary that maps words to indices and indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'king' :   2744\n",
      "Word which has index 2743:   kinds\n"
     ]
    }
   ],
   "source": [
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \",Ind2word[2743] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Training the Model\n",
    "\n",
    "<a name='2.1'></a>\n",
    "### 2.1 - Initializing the Model\n",
    "\n",
    "we will now initialize two matrices and two vectors. \n",
    "\n",
    "- The first matrix ($W_1$) is of dimension $N \\times V$, where\n",
    "    - $V$ is the number of words in the vocabulary \n",
    "    - and $N$ is the dimension of the word vector.\n",
    "- The second matrix ($W_2$) is of dimension $V \\times N$. \n",
    "- Vector $b_1$ has dimensions $N\\times 1$\n",
    "- Vector $b_2$ has dimensions  $V\\times 1$. \n",
    "- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n",
    "\n",
    "The overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### initialize_model\n",
    "we use [numpy.random.rand](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    W1 = np.random.rand(N, V)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    b1 = np.random.rand(N, 1)\n",
    "    b2 = np.random.rand(V, 1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Softmax\n",
    "Before we can start training the model, we need to implement the softmax function as defined in equation 5:  \n",
    "\n",
    "$$ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{i=0}^{V-1} e^{z_i}} $$\n",
    "\n",
    "\n",
    "- Array indexing in code starts at 0.\n",
    "- $V$ is the number of words in the vocabulary (which is also the number of rows of $z$).\n",
    "- $i$ goes from 0 to |V| - 1.\n",
    "\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### softmax\n",
    "the softmax function below does the following:\n",
    "\n",
    "- Assume that the input $z$ to `softmax` is a 2D array\n",
    "- Each training example is represented by a vector of shape (V, 1) in this 2D array.\n",
    "- There may be more than one column, in the 2D array, because we can put in a batch of examples to increase efficiency.  Let's call the batch size lowercase $m$, so the $z$ array has shape (V, m)\n",
    "- When taking the sum from $i=1 \\cdots V-1$, take the sum for each column (each example) separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    '''\n",
    "    Inputs: \n",
    "        z: output scores from the hidden layer\n",
    "    Outputs: \n",
    "        yhat: prediction (estimate of y)\n",
    "    '''\n",
    "    # Calculate yhat (softmax)\n",
    "    \n",
    "    e_z = np.exp(z - np.max(z, axis=0))  # Subtract max(z) for numerical stability\n",
    "    yhat = e_z / np.sum(e_z, axis=0)\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Forward Propagation\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### forward_prop\n",
    "Implement the forward propagation $z$ according to equations (1) to (3). <br>\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " h &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  h + b_2   \\tag{3} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For that, we will use as activation the Rectified Linear Unit (ReLU) given by:\n",
    "\n",
    "$$f(h)=\\max (0,h) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>we can use numpy.maximum(x1,x2) to get the maximum of two values</li>\n",
    "    <li>Use numpy.dot(A,B) to matrix multiply A and B</li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    \n",
    "    h = np.dot(W1, x) + b1\n",
    "  \n",
    "    # Apply the relu on h, \n",
    "    h = np.maximum(0, h)\n",
    "\n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 - Cost Function: *cross-entropy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.5'></a>\n",
    "### 2.5 - Training the Model - Backpropagation\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### back_prop\n",
    "Now that we have created a function for the forward propagation, we will implement a function that computes the gradients to backpropagate the errors.\n",
    "\n",
    "**Note**: `z1` is calculated as `W1⋅x + b1` in this function. In practice, we would save it already when making forward propagation and just re-use here, but for simplicity, it is calculated again in `back_prop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    \n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    \n",
    "    \n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    l1 = np.dot(W2.T, yhat - y)\n",
    "\n",
    "    # if z1 < 0, then l1 = 0\n",
    "    # otherwise l1 = l1\n",
    "    \n",
    "    l1[z1 < 0] = 0 # use \"l1\" to compute gradients below\n",
    "\n",
    "    # compute the gradient for W1\n",
    "    grad_W1 = np.dot(l1, x.T) / batch_size\n",
    "\n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n",
    "\n",
    "    # compute gradient for b2\n",
    "    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.6'></a>\n",
    "### 2.6 - Gradient Descent\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### gradient_descent\n",
    "Now that we have implemented a function to compute the gradients, we will implement batch gradient descent over your training set. \n",
    "\n",
    "**Hint:** For that, we will use `initialize_model` and the `back_prop` functions which you just created (and the `compute_cost` function). we can also use the provided `get_batches` helper function:\n",
    "\n",
    "```for x, y in get_batches(data, word2Ind, V, C, batch_size):```\n",
    "\n",
    "```...```\n",
    "\n",
    "Also: print the cost after each batch is processed (use batch size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, \n",
    "                    N, \n",
    "                    V,\n",
    "                    num_iters, \n",
    "                    alpha=0.03, \n",
    "                    random_seed=282, \n",
    "                    initialize_model=initialize_model, \n",
    "                    get_batches=get_batches, \n",
    "                    forward_prop=forward_prop, \n",
    "                    softmax=softmax, \n",
    "                    compute_cost=compute_cost, \n",
    "                    back_prop=back_prop):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: your implementation of the function to perform forward propagation\n",
    "        softmax: your implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: your implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) #W1=(N,V) and W2=(V,N)\n",
    "\n",
    "    batch_size = 128\n",
    "    iters = 0\n",
    "    C = 2 \n",
    "    \n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        # get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "                \n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "            \n",
    "        # get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 -= alpha * grad_W1\n",
    "        W2 -= alpha * grad_W2\n",
    "        b1 -= alpha * grad_b1\n",
    "        b2 -= alpha * grad_b2\n",
    "\n",
    "        iters +=1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c81df558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 9.513211\n",
      "iters: 20 cost: 9.476105\n",
      "iters: 30 cost: 9.543887\n",
      "iters: 40 cost: 9.370119\n",
      "iters: 50 cost: 8.752134\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "\n",
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Visualizing the Word Vectors\n",
    "\n",
    "In this part we will visualize the word vectors trained using the function we just coded above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
    "         'rich','happy','sad']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that man and king are next to each other. However, we have to be careful with the interpretation of this projected word vectors, since the PCA depends on the projection -- as shown in the following illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
